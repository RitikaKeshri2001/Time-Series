{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is meant by time-dependent seasonal components?\n",
    "\n",
    "Time-dependent seasonal components are the variations in a time series that occur with a fixed frequency and depend on the time of observation. It refer to recurring patterns or variations in a time series that are influenced by the time of year or season. In many time series data, such as sales data or temperature readings, there are regular fluctuations that occur at specific times of the year. The term \"time-dependent\" implies that the seasonal patterns change over time. Time-dependent seasonal components are different from time-independent seasonal components, which are constant over time and do not depend on the time of observation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. How can time-dependent seasonal components be identified in time series data?\n",
    "\n",
    "There are different methods to identify seasonal components in time series data:\n",
    "\n",
    "- Plotting and reviewing the data: This is a simple and intuitive way to check if there are any repeating patterns or cycles in the data that correspond to a fixed frequency, such as hour, day, week, month, year, etc. For example, we can plot the data with trend lines or zoom in on a specific period to see the seasonal variation.\n",
    "\n",
    "- Decomposing the data: This is a technique that separates the time series into different components, such as trend, seasonality, and residuals. There are different methods for decomposition, such as additive, multiplicative, or seasonal adjustment. By decomposing the data, we can isolate and examine the seasonal component more clearly.\n",
    "\n",
    "- Analyzing the autocorrelation function: This is a statistical tool that measures the correlation between a time series and its lagged values. The autocorrelation function can reveal the presence of seasonality by showing periodic spikes at certain lags that correspond to the seasonal frequency. For example, if the data has a yearly seasonality, we may see a spike at lag 12 for monthly data or lag 4 for quarterly data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What are the factors that can influence time-dependent seasonal components?\n",
    "\n",
    "Factors that can influence time-dependent seasonal components:\n",
    "\n",
    "- Climate or weather: This factor can affect the variation in variables such as temperature, rainfall, snowfall, etc. depending on the season of the year.\n",
    "\n",
    "- Customs or traditions: This factor can affect the variation in variables such as sales, consumption, production, etc. depending on the festivals, holidays, or events that occur at certain times of the year.\n",
    "\n",
    "- Biological or natural cycles: This factor can affect the variation in variables such as birth rate, death rate, migration, etc. depending on the natural rhythms of living organisms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. How are autoregression models used in time series analysis and forecasting?\n",
    "\n",
    "Autoregression models are a type of time-series models that use past values of a variable to predict its future values. They assume that the variable has a linear relationship with its own previous observations. For example, an autoregressive model of order 1 (AR(1)) can be written as:\n",
    "\n",
    "y<sub>t</sub> = y<sub>t-1</sub>Φ + c\n",
    "\n",
    "where y<sub>t</sub> is the value of the variable at time t, c is a constant, Φ is a coefficient, and c is a random error term.\n",
    "\n",
    "The general formula of an autoregressive model of order p (AR(p)) is:\n",
    "\n",
    "y<sub>t</sub> = y<sub>t-1</sub> Φ<sub>t-1</sub> + y<sub>t-2</sub> Φ<sub>t-2</sub> + ... + y<sub>t-n</sub> Φ<sub>t-n</sub> + c\n",
    "\n",
    "Autoregression models are useful for time series forecasting because they can capture the trend and seasonality patterns in the data. They can also measure the autocorrelation between the variable and its lagged values, which indicates how much the past influences the present."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. How do you use autoregression models to make predictions for future time points?\n",
    "\n",
    "To use autoregressive models to make predictions for future time points, we need to do the following steps:\n",
    "\n",
    "1. Explore time series data for autocorrelation: Autocorrelation is the correlation of a variable with itself at different lags. We can use plots or statistical tests to check if your data has significant autocorrelation at certain lags. This will help us determine the order of the autoregressive model, which is the number of lagged variables to use as input variables.\n",
    "\n",
    "2. Develop an autoregressive model and use it to make predictions: We can use libraries such as statsmodels or scikit-learn in Python to fit an autoregressive model to our data. We need to specify the order of the model and provide the training data as input. The model will learn the coefficients for each lagged variable and output a prediction for the next time step based on the input values.\n",
    "\n",
    "3. Use a developed autoregressive model to make rolling predictions: To make predictions for multiple future time steps, we can use a rolling or recursive strategy. This means that we use the predicted value for the next time step as an input for the following time step, and so on. We can also update the model with new observations as they become available."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. What is a moving average (MA) model and how does it differ from other time series models?\n",
    "\n",
    "A moving average (MA) model is a common approach for modeling univariate time series. It specifies that the output variable is cross-correlated with a non-identical to itself random-variable. In other words, it is a linear regression of the current value of the series against current and previous white noise error terms or random shocks. \n",
    "\n",
    "Moving average:\n",
    "\n",
    "y<sub>t</sub> = ∈<sub>t-1</sub> Φ<sub>t-1</sub> + ∈<sub>t-2</sub> Φ<sub>t-2</sub> + ... + ∈<sub>t-n</sub> Φ<sub>t-n</sub> + c\n",
    "\n",
    "A MA model differs from other time series models such as autoregressive (AR) models or autoregressive integrated moving average (ARIMA) models in several ways:\n",
    "\n",
    "- A MA model is always stationary, meaning that its mean, variance and autocorrelation do not change over time. In contrast, an AR model may or may not be stationary depending on its parameters, and an ARIMA model can handle non-stationary data by applying differencing. \n",
    "\n",
    "- A MA model has a finite impulse response, meaning that a shock affects values only for the current period and q periods into the future, where q is the order of the MA model. In contrast, an AR model has an infinite impulse response, meaning that a shock affects values infinitely far into the future. \n",
    "\n",
    "- A MA model can be fit by smoothing the time series curve by computing the average of all data points in a fixed-length window. This technique is known as moving average smoothing and can be used for data preparation, feature engineering, and forecasting. An ARIMA model can also be used for fitting a MA model by setting the AR and differencing terms to zero. Fitting a MA model is generally more complicated than fitting an AR model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. What is a mixed ARMA model and how does it differ from an AR or MA model?\n",
    "\n",
    "A mixed ARMA model is a model that combines the features of both autoregressive (AR) and moving average (MA) models. It specifies that the output variable is a linear function of its own past values and past white noise error terms.\n",
    "\n",
    "A mixed ARMA model differs from an AR or MA model in several ways:\n",
    "\n",
    "- A mixed ARMA model can capture both short-term and long-term dependencies in the data, while an AR model can only capture long-term dependencies and an MA model can only capture short-term dependencies. \n",
    "- A mixed ARMA model can account for both predictable patterns and unpredictable shocks in the data, while an AR model can only account for predictable patterns and an MA model can only account for unpredictable shocks.\n",
    "- A mixed ARMA model can be more flexible and parsimonious than an AR or MA model, as it can fit a wide range of data with fewer parameters. \n",
    "\n",
    "A mixed ARMA model is defined by two orders: p and q, where p is the order of the AR part and q is the order of the MA part. The general form of a mixed ARMA(p,q) model is:\n",
    "\n",
    "y<sub>t</sub> = y<sub>t-1</sub> Φ<sub>t-1</sub> + y<sub>t-2</sub> Φ<sub>t-2</sub> + ... + y<sub>t-n</sub> Φ<sub>t-n</sub> + ∈<sub>t-1</sub> Φ<sub>t-1</sub> + ∈<sub>t-2</sub> Φ<sub>t-2</sub> + ... + ∈<sub>t-n</sub> Φ<sub>t-n</sub> + c\n",
    "\n",
    "where y t is the output variable, c is a constant,ϵ<sub>t</sub> are the white noise error terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
